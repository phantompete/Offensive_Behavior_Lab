{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f62c6b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33cf83",
   "metadata": {},
   "source": [
    "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Oracle_logo.svg/2560px-Oracle_logo.svg.png\" width=\"200\" align = \"left\"></p>\n",
    "\n",
    "# **<h1 align =\"middle\"><b> Oracle CloudWorld - Las Vegas</b></h1>**\n",
    "\n",
    "### **<h1 align =\"middle\"><b> Use case 1. Person Detection in Video</b></h1>**\n",
    "### **<h1 align =\"middle\"><b> Use case 2. Offensive Language Detection in Video</b></h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b480005f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f949a91",
   "metadata": {},
   "source": [
    "#### In this notebook, all the individual .py files are listed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ec4b3",
   "metadata": {},
   "source": [
    "# **| 1. Video_only.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da985aa",
   "metadata": {},
   "source": [
    "## **1.1 Script video_only.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb252ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/datascience/job_artifacts/sub_packages/video_only.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/datascience/ocw_las_vegas/job_artifacts/sub_packages/video_only.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from deepface import DeepFace\n",
    "import uuid\n",
    "import glob      \n",
    "import ocifs\n",
    "import base64\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import fsspec\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pytube import YouTube\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import oci\n",
    "import json\n",
    "import ocifs\n",
    "import time\n",
    "from oci.object_storage import ObjectStorageClient\n",
    "from oci.ai_language import AIServiceLanguageClient\n",
    "from oci.ai_language.models import DetectLanguageKeyPhrasesDetails\n",
    "from oci.ai_language.models import DetectLanguageSentimentsDetails\n",
    "from oci.ai_speech import AIServiceSpeechClient\n",
    "from oci.ai_speech.models import TranscriptionModelDetails\n",
    "from oci.ai_speech.models import ObjectLocation\n",
    "from oci.ai_speech.models import ObjectListInlineInputLocation\n",
    "from oci.ai_speech.models import OutputLocation\n",
    "from oci.ai_speech.models import CreateTranscriptionJobDetails\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "##########################################################################################################################################\n",
    "######################################################## Use case 1          #############################################################\n",
    "##########################################################################################################################################\n",
    "##########################################################################################################################################\n",
    "\n",
    "##########################################################################################################################################\n",
    "######################################################## Function 1          #############################################################\n",
    "##########################################################################################################################################\n",
    "\n",
    "def input_youtube_video(YOUTUBE_URL):\n",
    "    \n",
    "    #delete previous videos\n",
    "    os.system(\"rm -r /home/datascience/youtube_videos\")\n",
    "\n",
    "    #create a local directory to store the video\n",
    "    path_input_locally = \"/home/datascience/youtube_videos/\" \n",
    "\n",
    "    try:       \n",
    "        if not os.path.exists(path_input_locally):         \n",
    "            os.makedirs(path_input_locally)    \n",
    "\n",
    "    except OSError: \n",
    "        print ('Error: Creating directory for youtube video locally')\n",
    "        \n",
    "\n",
    "    #download file from youtube\n",
    "    yt = YouTube(YOUTUBE_URL)\n",
    "\n",
    "    #store in local folder\n",
    "    stream = yt.streams.get_by_itag(22)\n",
    "    file_name_random = str(uuid.uuid4())\n",
    "    file_location_local = stream.download(output_path=path_input_locally, filename  = file_name_random + \".mp4\")\n",
    "    \n",
    "    print(\"Youtube download completed and stored in \" + str(file_location_local))\n",
    "    \n",
    "    return file_location_local\n",
    "\n",
    "##########################################################################################################################################\n",
    "######################################################## Function 2          #############################################################\n",
    "##########################################################################################################################################\n",
    "\n",
    "def input_profile_image(full_bucket_name):\n",
    "    \n",
    "    #create a local directory to store the image\n",
    "    path_input_locally_image = \"/home/datascience/profile_image/\" \n",
    "\n",
    "    try:       \n",
    "        if not os.path.exists(path_input_locally_image):         \n",
    "            os.makedirs(path_input_locally_image)    \n",
    "\n",
    "    except OSError: \n",
    "        print ('Error: Creating directory for profile image locally')\n",
    "\n",
    "    print(\"Full bucket name is \" + full_bucket_name)\n",
    "    \n",
    "    #get the image from the bucket and store locally\n",
    "    fs = ocifs.OCIFileSystem()\n",
    "    fs.invalidate_cache(full_bucket_name)\n",
    "    fs.get((full_bucket_name + \"*.jpg\"), path_input_locally_image , recursive=True, refresh=True)\n",
    "    \n",
    "    #get file name    \n",
    "    file = [os.path.basename(x) for x in glob.glob(path_input_locally_image + '*.jpg')]\n",
    "    profile_image_name = file[0]\n",
    "    profile_image_loc = path_input_locally_image + file[0]\n",
    "    print(\"Profile image is stored locally at \" + profile_image_loc)\n",
    "        \n",
    "    #delete input image from bucket to clear the bucket\n",
    "    delete_object = os.path.join(full_bucket_name, file[0])\n",
    "    fs.rm(delete_object, recursive=True)\n",
    "    \n",
    "    print(\"Image stored locally and removed from bucket\")\n",
    "    \n",
    "    return profile_image_name, profile_image_loc\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "######################################################## Predict             #############################################################\n",
    "##########################################################################################################################################\n",
    "\n",
    "def predict_video(full_bucket_name, YOUTUBE_URL, SCHEMA_NAME):\n",
    "    \n",
    "    # Delete images if there are images in the local folder already\n",
    "    path_split_images = \"/home/datascience/split_images\"\n",
    "    files = glob.glob('/home/datascience/split_images/*.jpg')\n",
    "\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "     #create a local folder to the images\n",
    "    path_split_images = \"/home/datascience/split_images\"\n",
    "\n",
    "    try:       \n",
    "        # creating a folder named split_images \n",
    "        if not os.path.exists(path_split_images):         \n",
    "            os.makedirs(path_split_images)    \n",
    "\n",
    "    except OSError: \n",
    "        print ('Error: Creating directory of data for split images')\n",
    "        \n",
    "    ######\n",
    "    ###### Function 1    \n",
    "    file_location_local = input_youtube_video(YOUTUBE_URL)\n",
    "    \n",
    "    ######\n",
    "    ###### Function 2\n",
    "    profile_image_name, profile_image_loc = input_profile_image(full_bucket_name)  \n",
    "    \n",
    "    print(\"Fetching video from \" + file_location_local)\n",
    "    print(\"Fetching profile image from \" + profile_image_loc)\n",
    "    \n",
    "    #####\n",
    "    ##### Main function video_only\n",
    "    \n",
    "    # Read the video from specified path \n",
    "    cam = cv2.VideoCapture(file_location_local)\n",
    "    \n",
    "    #get fps of original video\n",
    "    fps = cam.get(cv2.CAP_PROP_FPS)\n",
    "    number_of_frames = cam.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    print(\"**************************************************************** Original fps in video is \" + str(fps))\n",
    "    \n",
    "    # calculate duration of the video\n",
    "    total_duration_video = round(number_of_frames / fps)\n",
    "    \n",
    "    #define list of frames to analyze. \n",
    "    list_of_frames = list(range(1, 9001, 30))  #max 5 minutes of video. #starts at frame 1, ends at frame 601 (which is 20 seconds at FPS = 30 and 10 seconds at FPS = 60) with 30 frames in between (= 1 second). So, takes 20 (= 21 seconds) frames from the video\n",
    "\n",
    "    #loop through the video and cut into images\n",
    "    currentframe = 0\n",
    "\n",
    "    while(True):\n",
    "        \n",
    "        for frame in list_of_frames:\n",
    "            cam.set(cv2.CAP_PROP_POS_FRAMES, frame)\n",
    "            print(\"Analyze frame number \" + str(frame))\n",
    "\n",
    "            # reading from frame \n",
    "            ret,frame = cam.read()\n",
    "\n",
    "            if ret:\n",
    "                if currentframe < 10:   \n",
    "                    name = path_split_images + '/frame000' + str(currentframe) + '.jpg'           \n",
    "\n",
    "                elif currentframe >= 10 and currentframe < 100:   \n",
    "                    name = path_split_images + '/frame00' + str(currentframe) + '.jpg'          \n",
    "\n",
    "                elif currentframe >= 100 and currentframe < 1000:   \n",
    "                    name = path_split_images + '/frame0' + str(currentframe) + '.jpg'   \n",
    "\n",
    "                else:\n",
    "                    name = path_split_images + '/frame' + str(currentframe) + '.jpg'      \n",
    "\n",
    "                print ('Creating...' + name) \n",
    "\n",
    "                # writing the extracted images \n",
    "                cv2.imwrite(name, frame) \n",
    "\n",
    "                # increasing counter\n",
    "                currentframe += 1\n",
    "            \n",
    "        else: \n",
    "            break\n",
    "\n",
    "    cam.release()    \n",
    "    \n",
    "    #apply DeepFace to the images\n",
    "    try:\n",
    "        dfs = DeepFace.find(img_path = profile_image_loc, db_path = \"/home/datascience/split_images\", enforce_detection=False)  #first input is the profile image, second is the folder containing the split images\n",
    "        \n",
    "    except:\n",
    "        pass #if no face is found in any of the images\n",
    "    \n",
    "    #get the dataframe of the results\n",
    "    output_df = dfs[0]\n",
    "    \n",
    "    ########## calculations\n",
    "    seconds_in_screen = output_df.shape[0]  # = total frames detected = frames per second as we are looping through each frame\n",
    "    total_seconds_video_analyzed = total_duration_video\n",
    "    \n",
    "    #in screen calc\n",
    "    list_in_screen = []\n",
    "    if seconds_in_screen > 0:\n",
    "        in_screen = 'Person was in video'\n",
    "        print(in_screen)\n",
    "        list_in_screen.append(in_screen)\n",
    "    else:\n",
    "        in_screen = 'Person was not in video'\n",
    "        list_in_screen.append(in_screen)\n",
    "        print(in_screen)\n",
    "        \n",
    "    output_in_screen = list_in_screen[0]\n",
    "    \n",
    "    print(\"**************************************************************** Person was or was not in screen is :\" + str(output_in_screen))\n",
    "    print(\"**************************************************************** Total seconds analyzed of entire video \" + str(len(list_of_frames)) + \" seconds\")\n",
    "    print(\"**************************************************************** This person was \" + str(seconds_in_screen) + \" seconds in screen\")\n",
    "    \n",
    "    return output_in_screen, seconds_in_screen, total_seconds_video_analyzed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f3418",
   "metadata": {},
   "source": [
    "## **1.2 Test Script video_only.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bucket_name = \"West_BP\"\n",
    "YOUTUBE_URL = \"https://www.youtube.com/shorts/ugwUcdtygok\"\n",
    "SCHEMA_NAME = \"ocw\"\n",
    "full_bucket_name = \"oci://West_BP@frqap2zhtzbe/ocw\"\n",
    "output_in_screen, seconds_in_screen, total_seconds_video_analyzed  = predict_video(main_bucket_name, namespace, YOUTUBE_URL, BUCKET_NAME, SCHEMA_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63860f7c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebc5079",
   "metadata": {},
   "source": [
    "# **2. Audio_only.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeca6a6",
   "metadata": {},
   "source": [
    "## **2.1 Script Audio_only.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd3be316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/datascience/ocw_las_vegas/job_artifacts/sub_packages/audio_only.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/datascience/ocw_las_vegas/job_artifacts/sub_packages/audio_only.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import uuid\n",
    "import glob      \n",
    "import ocifs\n",
    "import base64\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import fsspec\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from ads.model.framework.tensorflow_model import TensorFlowModel\n",
    "from ads.common.model_metadata import UseCaseType\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "from pytube import YouTube\n",
    "import oci\n",
    "import json\n",
    "import time\n",
    "from oci.object_storage import ObjectStorageClient\n",
    "from oci.ai_language import AIServiceLanguageClient\n",
    "from oci.ai_language.models import DetectLanguageKeyPhrasesDetails\n",
    "from oci.ai_language.models import DetectLanguageSentimentsDetails\n",
    "from oci.ai_speech import AIServiceSpeechClient\n",
    "from oci.ai_speech.models import TranscriptionModelDetails\n",
    "from oci.ai_speech.models import ObjectLocation\n",
    "from oci.ai_speech.models import ObjectListInlineInputLocation\n",
    "from oci.ai_speech.models import OutputLocation\n",
    "from oci.ai_speech.models import CreateTranscriptionJobDetails\n",
    "\n",
    "##########################################################################################################################################\n",
    "######################################################## Function 1          #############################################################\n",
    "##########################################################################################################################################\n",
    "\n",
    "def input_youtube_video_audio(YOUTUBE_URL):\n",
    "    \n",
    "    #delete previous videos\n",
    "    os.system(\"rm -r /home/datascience/youtube_videos_audio\")\n",
    "\n",
    "    #create a local directory to store the video\n",
    "    path_input_locally = \"/home/datascience/youtube_videos_audio/\" \n",
    "\n",
    "    try:       \n",
    "        if not os.path.exists(path_input_locally):         \n",
    "            os.makedirs(path_input_locally)    \n",
    "\n",
    "    except OSError: \n",
    "        print ('Error: Creating directory for youtube audio locally')\n",
    "        \n",
    "\n",
    "    #download file from youtube\n",
    "    yt = YouTube(YOUTUBE_URL)\n",
    "\n",
    "    #store in local folder\n",
    "    stream = yt.streams.get_by_itag(139)  #139 is audio only\n",
    "    file_name_random = str(uuid.uuid4())\n",
    "    file_location_local_audio = stream.download(output_path=path_input_locally, filename  = file_name_random + \".mp4\")\n",
    "    \n",
    "    print(\"Youtube download for audio only completed and stored in \" + str(file_location_local_audio))\n",
    "    \n",
    "    return file_location_local_audio\n",
    "\n",
    "##########################################################################################################################################\n",
    "######################################################## Function 2          #############################################################\n",
    "##########################################################################################################################################\n",
    "\n",
    "def audio_to_object_storage(bucket_name_input, namespace_input, config, name, file_location_local):\n",
    "    \n",
    "    #Object Storage Client\n",
    "    client_object_storage = ObjectStorageClient(config)\n",
    "    \n",
    "    #Define bucket and namespace\n",
    "    bucket_name = bucket_name_input\n",
    "    namespace = namespace_input\n",
    "\n",
    "    #Audio to Bucket\n",
    "    response = client_object_storage.put_object(namespace, bucket_name, name, io.open(file_location_local, 'rb'), content_type='audio/wav')\n",
    "\n",
    "##########################################################################################################################################\n",
    "######################################################## Function 3          #############################################################\n",
    "##########################################################################################################################################\n",
    "\n",
    "def run_speech_model(bucket_name_input, namespace_input, compartment_id_input, config, name):\n",
    "    \n",
    "    # Instantiate Speech Client\n",
    "    ai_speech_client = AIServiceSpeechClient(config)\n",
    "    \n",
    "    # Define Parameters for Transcription Jobs\n",
    "    job_display_name = \"Offensive_Language_Detection\"\n",
    "    job_compartment_id = compartment_id_input\n",
    "    job_description = \"Offensive_Language_Detection\"\n",
    "    bucket_name = bucket_name_input\n",
    "    namespace = namespace_input\n",
    "    output_prefix = \"speech_out_\"\n",
    "    \n",
    "\n",
    "    # Define Transcription Job - Model, Data, Input, Outputs\n",
    "    job_model_details = TranscriptionModelDetails(domain=\"GENERIC\", language_code=\"en-GB\")\n",
    "    job_object_location = ObjectLocation(namespace_name=namespace, bucket_name=bucket_name,object_names=[name])\n",
    "    job_input_location = ObjectListInlineInputLocation(location_type=\"OBJECT_LIST_INLINE_INPUT_LOCATION\", object_locations=[job_object_location])\n",
    "    job_output_location = OutputLocation(namespace_name=namespace, bucket_name=bucket_name, prefix=output_prefix)\n",
    "\n",
    "    \n",
    "    # Create Transcription Job with details provided above\n",
    "    transcription_job_details = CreateTranscriptionJobDetails(display_name=job_display_name,\n",
    "                                                                compartment_id=job_compartment_id,\n",
    "                                                                description=job_description,\n",
    "                                                                model_details=job_model_details,\n",
    "                                                                input_location=job_input_location,\n",
    "                                                                output_location=job_output_location)\n",
    "\n",
    "    \n",
    "    # Call the AI Speech Service to Create Transcription Job \n",
    "    transcription_job = None\n",
    "    try:\n",
    "        transcription_job = ai_speech_client.create_transcription_job(create_transcription_job_details=transcription_job_details)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        print(transcription_job.data.lifecycle_state)\n",
    "        \n",
    "    # Pause for 3 Seconds to Allow Job to be Accepted\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Gets the First Transcription Tasks under given Transcription Job Id then Extracts Info for that Task\n",
    "    transcription_tasks = None\n",
    "    try:\n",
    "        # Get Tasks Under Job\n",
    "        transcription_tasks = ai_speech_client.list_transcription_tasks(transcription_job.data.id, limit=1)\n",
    "        \n",
    "        # Keep Checking until Task is Succeeded\n",
    "        while transcription_tasks.data.items[0].lifecycle_state != 'SUCCEEDED':\n",
    "            print('Transcribing in Progress...')\n",
    "            time.sleep(5)\n",
    "            transcription_tasks = ai_speech_client.list_transcription_tasks(transcription_job.data.id, limit=1)\n",
    "            \n",
    "        # Once Task is Succeeded Extract Task Info\n",
    "        transcription_task = ai_speech_client.get_transcription_task(transcription_job.data.id, transcription_tasks.data.items[0].id)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    else:\n",
    "        print(transcription_tasks.data.items[0].lifecycle_state)\n",
    "        print(transcription_task.data.output_location.object_names[0])\n",
    "    \n",
    "    # Extract Results File Name from Task Info Response\n",
    "    object_name = transcription_task.data.output_location.object_names[0]\n",
    "    \n",
    "    return object_name\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "######################################################## Function 4          #############################################################\n",
    "##########################################################################################################################################\n",
    "\n",
    "def parse_results(bucket_name_input, namespace_input, config, object_name):\n",
    "    \n",
    "    # Instantiate Object Storage Client\n",
    "    client = ObjectStorageClient(config)\n",
    "    \n",
    "    # Define Parameters\n",
    "    bucket_name = bucket_name_input\n",
    "    namespace = namespace_input\n",
    "    \n",
    "    # Get Speech Results File from Object Storage\n",
    "    response = client.get_object(namespace, bucket_name, object_name)\n",
    "    \n",
    "    # Decode Results from File\n",
    "    decoded_resp = json.loads(response.data.content.decode())\n",
    "    \n",
    "    # Extract Transcription from Results\n",
    "    transcription_out = decoded_resp['transcriptions'][0]['transcription']\n",
    "    print(transcription_out)\n",
    "    return transcription_out\n",
    "\n",
    "##########################################################################################################################################\n",
    "######################################################## Function 5          #############################################################\n",
    "##########################################################################################################################################\n",
    "\n",
    "def run_language_models(config, transcription_out):\n",
    "    \n",
    "    # Initialize Service Client to Language API\n",
    "    ai_language_client = AIServiceLanguageClient(config)\n",
    "    \n",
    "    \n",
    "    # Make a REST API Request to AI Language Service to Detect Key Phrases\n",
    "    language_key_phrases = ai_language_client.detect_language_key_phrases(\n",
    "        detect_language_key_phrases_details=DetectLanguageKeyPhrasesDetails(text = transcription_out))\n",
    "    \n",
    "    # Results List\n",
    "    key_phrase_results = []\n",
    "    \n",
    "    # Extract Language Entities\n",
    "    formatted_response = language_key_phrases.data.key_phrases\n",
    "    \n",
    "    # Iterate through and Store Entites in Results List\n",
    "    for key_phrase in formatted_response:\n",
    "        key_phrase_results.append(key_phrase.text)\n",
    "        \n",
    "    #merge key phrases extracted from list in one string: \n",
    "    key_phrases_string_out = 'input: '\n",
    "    for key_phrase in key_phrase_results:\n",
    "        key_phrases_string_out += key_phrase + ', '\n",
    "        \n",
    "    ##maximuze input for sentiment and for push to db\n",
    "    key_phrases_string = key_phrases_string_out[0:498]  #filter on max 500 characters\n",
    "\n",
    "    # Make a REST API Request to AI Language Service to Detect Sentiments\n",
    "    language_sentiment_response = ai_language_client.detect_language_sentiments(\n",
    "        detect_language_sentiments_details=DetectLanguageSentimentsDetails(text = key_phrases_string))\n",
    "    \n",
    "    # Results List\n",
    "    sentiment_results = []\n",
    "    \n",
    "    # Extract Language Sentiments\n",
    "    formatted_response = language_sentiment_response.data.aspects\n",
    "    \n",
    "    # Iterate through and Store Aspect Sentiment in Results List\n",
    "    for aspect in formatted_response:\n",
    "        sentiment_results.append((aspect.text, aspect.sentiment))\n",
    "\n",
    "    return key_phrases_string, sentiment_results\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "######################################################## Predict             #############################################################\n",
    "##########################################################################################################################################\n",
    "\n",
    "\n",
    "def predict_audio(YOUTUBE_URL, MAIN_BUCKET_NAME, NAMESPACE_NAME, compartment_id_input):\n",
    "    \n",
    "    main_bucket_name = MAIN_BUCKET_NAME\n",
    "    namespace = NAMESPACE_NAME\n",
    "        \n",
    "    # Authenticate against OCI \n",
    "    config = oci.config.from_file('config', 'DEFAULT')                                                       #### ****** -------- In Job\n",
    "    #config = oci.config.from_file('/home/datascience/.oci/config', 'DEFAULT')                               #### ****** -------- In notebook\n",
    "    \n",
    "    #Function 1. Download YouTube video as recording\n",
    "    file_location_local_audio = input_youtube_video_audio(YOUTUBE_URL)\n",
    "    \n",
    "    # Send Audio File to Object storage\n",
    "    name = 'offensive_language.wav'\n",
    "    audio_to_object_storage(main_bucket_name, namespace, config, name, file_location_local_audio) #passing fixed main bucket name and namespace. These are in main.py defined\n",
    "    \n",
    "    # Run Speech Model - Returns Results object\n",
    "    object_name = run_speech_model(main_bucket_name, namespace, compartment_id_input, config, name)\n",
    "    \n",
    "    # Get Results File from Object Storage and Parse Transcription\n",
    "    transcription_out = parse_results(main_bucket_name, namespace, config, object_name)\n",
    "    transcription = transcription_out[0:498]  #filter on max 500 characters\n",
    "    \n",
    "    # Run Language Models on Transcription to Get Key Phrases and Sentiment\n",
    "    key_phrases_string, sentiment_results = run_language_models(config, transcription_out)\n",
    "    \n",
    "    \n",
    "    #convert list to string\n",
    "    sentiment_result_string_output = 'input: '\n",
    "    for sentimentx in sentiment_results:\n",
    "        sentiment_result_string_output += sentimentx[0] + ', '\n",
    "    \n",
    "    sentiment_result_string = sentiment_result_string_output[0:498]  #filter on max 500 characters\n",
    "        \n",
    "    # Count Negative Aspects\n",
    "    neg_aspects = 0 \n",
    "    \n",
    "    for sentiment in sentiment_results:\n",
    "        if sentiment[1] == 'Negative':\n",
    "            neg_aspects += 1\n",
    "\n",
    "    \n",
    "    return transcription, key_phrases_string, sentiment_result_string, neg_aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7854d0",
   "metadata": {},
   "source": [
    "## **2.2 Test Script Audio_only.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da60e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#below are added to main.py\n",
    "main_bucket_name = \"West_BP\"\n",
    "namespace = \"frqap2zhtzbe\"\n",
    "compartment_id_input = \"ocid1.compartment.oc1..aaaaaaaae3n6r6hrjipbap2hojicrsvkzatrtlwvsyrpyjd7wjnw4za3m75q\"\n",
    "\n",
    "\n",
    "YOUTUBE_URL = \"https://www.youtube.com/shorts/05ldl6tfJ78\" \n",
    "#other example: https://www.youtube.com/shorts/Y-PBRyEz4xY\n",
    "\n",
    "transcription, key_phrases_string, sentiment_result_string, neg_aspects = predict_audio(YOUTUBE_URL, main_bucket_name, namespace, compartment_id_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26307d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b22e4",
   "metadata": {},
   "source": [
    "# **3. Main.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a99bf",
   "metadata": {},
   "source": [
    "## **3.1 Script main.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a7c0050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/datascience/ocw_las_vegas/job_artifacts/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/datascience/ocw_las_vegas/job_artifacts/main.py\n",
    "\n",
    "### input variables for start of workshop\n",
    "####################################\n",
    "#################################### CHANGE THE BELOW PARAMETERS\n",
    "####################################\n",
    "####################################\n",
    "\n",
    "password = \"\"                            # Database password for user name\n",
    "wallet_name = \"\"                         # The name of wallet or database, like \"DB202112101358\", excluding \"Wallet_\" and excluding \".zip\"\n",
    "compartment_id_input = \"\"                # OCID of the comparment\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "#################################### DO NOT CHANGE THE BELOW.\n",
    "####################################\n",
    "####################################\n",
    "\n",
    "wallet_storage_directory = './wallet'           \n",
    "\n",
    "#imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from deepface import DeepFace\n",
    "import uuid\n",
    "import glob      \n",
    "import ocifs\n",
    "import base64\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import fsspec\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pytube import YouTube\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import oci\n",
    "import json\n",
    "import ocifs\n",
    "import time\n",
    "from oci.object_storage import ObjectStorageClient\n",
    "from oci.ai_language import AIServiceLanguageClient\n",
    "from oci.ai_language.models import DetectLanguageKeyPhrasesDetails\n",
    "from oci.ai_language.models import DetectLanguageSentimentsDetails\n",
    "from oci.ai_speech import AIServiceSpeechClient\n",
    "from oci.ai_speech.models import TranscriptionModelDetails\n",
    "from oci.ai_speech.models import ObjectLocation\n",
    "from oci.ai_speech.models import ObjectListInlineInputLocation\n",
    "from oci.ai_speech.models import OutputLocation\n",
    "from oci.ai_speech.models import CreateTranscriptionJobDetails\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import cx_Oracle\n",
    "import ads\n",
    "\n",
    "#import custom py files\n",
    "from sub_packages.video_only import input_youtube_video\n",
    "from sub_packages.video_only import input_profile_image\n",
    "from sub_packages.video_only import predict_video\n",
    "from sub_packages.audio_only import input_youtube_video_audio\n",
    "from sub_packages.audio_only import audio_to_object_storage\n",
    "from sub_packages.audio_only import run_speech_model\n",
    "from sub_packages.audio_only import parse_results\n",
    "from sub_packages.audio_only import run_language_models\n",
    "from sub_packages.audio_only import predict_audio\n",
    "from sub_packages.credentials import create_uri\n",
    "from sub_packages.roberta import preprocess\n",
    "from sub_packages.roberta import roberta_base\n",
    "from sub_packages.check_table import check_table\n",
    "\n",
    "print(\"Main imports done\")\n",
    "\n",
    "#fetch environment variables from APEX input. The below are default values.\n",
    "TYPE_OF_ANALYSIS = os.environ.get(\"TYPE_OF_ANALYSIS\", \"both\")\n",
    "YOUTUBE_URL = os.environ.get(\"YOUTUBE_URL\", \"https://www.youtube.com/shorts/ugwUcdtygok\")\n",
    "MAIN_BUCKET_NAME = os.environ.get(\"MAIN_BUCKET_NAME\", \"West_BP\")\n",
    "NAMESPACE_NAME = os.environ.get(\"NAMESPACE_NAME\", \"frqap2zhtzbe\")\n",
    "#SUB_BUCKET_NAME = os.environ.get(\"SUB_BUCKET_NAME\", \"las_vegas\")\n",
    "SCHEMA_NAME = os.environ.get(\"SCHEMA_NAME\", \"test\")\n",
    "\n",
    "#Get job run ocid\n",
    "JOB_RUN_OCID_KEY = \"JOB_RUN_OCID\"\n",
    "job_run_ocid = os.environ.get(JOB_RUN_OCID_KEY, \"UNDEFINED\")\n",
    "\n",
    "print(\"Type of analysis is \" + TYPE_OF_ANALYSIS)\n",
    "print(\"Youtube URL is \" + YOUTUBE_URL)\n",
    "print(\"Bucket name is \" + MAIN_BUCKET_NAME)\n",
    "print(\"Schema name \" + SCHEMA_NAME)\n",
    "\n",
    "#get full bucket, namespace, sub_bucket directory\n",
    "full_bucket_name = \"oci://\"+MAIN_BUCKET_NAME+\"@\"+NAMESPACE_NAME+\"/\"+SCHEMA_NAME+\"/\"\n",
    "\n",
    "#route type of analysis\n",
    "\n",
    "if TYPE_OF_ANALYSIS == 'video_only':\n",
    "    output_in_screen, seconds_in_screen, total_seconds_video_analyzed = predict_video(full_bucket_name, YOUTUBE_URL, SCHEMA_NAME)\n",
    "    transcription = 'no_transcription'\n",
    "    key_phrases_string = 'no_result'\n",
    "    sentiment_result_string = 'no_result'\n",
    "    neg_aspects = 0\n",
    "    non_offensive = 0   #default values\n",
    "    offensive = 0\n",
    "    non_hate = 0\n",
    "    hate = 0\n",
    "    \n",
    "elif TYPE_OF_ANALYSIS == 'audio_only':\n",
    "    transcription, key_phrases_string, sentiment_result_string, neg_aspects = predict_audio(YOUTUBE_URL, MAIN_BUCKET_NAME, NAMESPACE_NAME, compartment_id_input)\n",
    "    new_text = preprocess(transcription)\n",
    "    non_offensive, offensive, non_hate, hate = roberta_base(new_text)\n",
    "    output_in_screen = \"no_video\"\n",
    "    seconds_in_screen = 0\n",
    "    total_seconds_video_analyzed = 0\n",
    "\n",
    "elif TYPE_OF_ANALYSIS == 'both':\n",
    "    output_in_screen, seconds_in_screen, total_seconds_video_analyzed = predict_video(full_bucket_name, YOUTUBE_URL, SCHEMA_NAME)  #video\n",
    "    transcription, key_phrases_string, sentiment_result_string, neg_aspects = predict_audio(YOUTUBE_URL, MAIN_BUCKET_NAME, NAMESPACE_NAME, compartment_id_input)  #audio\n",
    "    new_text = preprocess(transcription)   # cleans text for input roberta model\n",
    "    non_offensive, offensive, non_hate, hate = roberta_base(new_text)   #roberta model\n",
    "    \n",
    "else: #default values if job fails\n",
    "    output_in_screen = \"unclear\"\n",
    "    seconds_in_screen = 0\n",
    "    total_seconds_video_analyzed = 0\n",
    "    transcription = 'no_transcription'\n",
    "    key_phrases_string = 'no_result'\n",
    "    sentiment_result_string = 'no_result'\n",
    "    neg_aspects = 0\n",
    "    non_offensive = 1   #default values\n",
    "    offensive = 1\n",
    "    non_hate = 1\n",
    "    hate = 1\n",
    "\n",
    "#change floats to integers\n",
    "non_offensive_int = int(round((non_offensive * 100),0))\n",
    "offensive_int = int(round((offensive * 100),0))\n",
    "non_hate_int = int(round((non_hate * 100),0))\n",
    "hate_int = int(round((hate * 100),0))\n",
    "    \n",
    "    \n",
    "#Construct all variables in a single line as PD dataframe\n",
    "data = [[job_run_ocid, TYPE_OF_ANALYSIS, output_in_screen, seconds_in_screen, total_seconds_video_analyzed, \n",
    "         transcription, key_phrases_string, sentiment_result_string, neg_aspects, \n",
    "        non_offensive_int, offensive_int, non_hate_int, hate_int]]\n",
    "\n",
    "df_resultsx = pd.DataFrame(data, columns = ['job_run_ocid', 'type_of_analysis', 'output_in_screen', 'seconds_in_screen', 'total_seconds_video_analyzed',\n",
    "                                           'transcription', 'key_phrases_string', 'sentiment_result_string', 'neg_aspects',\n",
    "                                           'non_offensive_int', 'offensive_int', 'non_hate_int', 'hate_int'])\n",
    "\n",
    "\n",
    "\n",
    "print(\"End of audio and video. Results are printed below\")\n",
    "print(seconds_in_screen)\n",
    "print(total_seconds_video_analyzed)\n",
    "print(transcription)\n",
    "print(key_phrases_string)\n",
    "print(sentiment_result_string)\n",
    "    \n",
    "#### create uri using credentials.py and create engine\n",
    "engine, wallet_filename = create_uri(SCHEMA_NAME, password, wallet_name, wallet_storage_directory)                                                                #------------------xxxxxxxxxxxxxxxxxx CHANGE user_name in notebook, SCHEMA_NAME in Job\n",
    "#engine = create_uri(user_name, password, wallet_name, wallet_storage_directory)   \n",
    "print(engine)\n",
    "\n",
    "#check whether table exists already or not. Will be 'replace' or 'append'\n",
    "table_status = check_table(SCHEMA_NAME, password, wallet_name, wallet_filename)\n",
    "\n",
    "## push results to database\n",
    "df_resultsx.to_sql('ocw_run_results', con=engine, index=False, if_exists=table_status, dtype={\n",
    "    \n",
    "            'job_run_ocid': sqlalchemy.types.NVARCHAR(length=500),    ## from video\n",
    "            'type_of_analysis': sqlalchemy.types.NVARCHAR(length=500),\n",
    "            'output_in_screen': sqlalchemy.types.NVARCHAR(length=500),\n",
    "            'seconds_in_screen': sqlalchemy.types.INTEGER(),\n",
    "            'total_seconds_video_analyzed': sqlalchemy.types.INTEGER(),\n",
    "    \n",
    "            'transcription': sqlalchemy.types.NVARCHAR(length=500),     ## from audio\n",
    "            'key_phrases_string': sqlalchemy.types.NVARCHAR(length=500),\n",
    "            'sentiment_result_string': sqlalchemy.types.NVARCHAR(length=500),\n",
    "            'neg_aspects': sqlalchemy.types.INTEGER(),\n",
    "    \n",
    "            'non_offensive_int': sqlalchemy.types.INTEGER(), #roberta\n",
    "            'offensive_int': sqlalchemy.types.INTEGER(), \n",
    "            'non_hate_int': sqlalchemy.types.INTEGER(), \n",
    "            'hate_int': sqlalchemy.types.INTEGER() })\n",
    "\n",
    "\n",
    "print(\"Table replaced and updated with results\")\n",
    "print(\"----------- The end -----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43637d3c",
   "metadata": {},
   "source": [
    "## **Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ac57c1",
   "metadata": {},
   "source": [
    "## **3.2 Test Script main.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c71ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a4317",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981f36d2",
   "metadata": {},
   "source": [
    "# **4. roberta.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7687883a",
   "metadata": {},
   "source": [
    "## **4.1 Script roberta.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7692be09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/datascience/ocw_las_vegas/job_artifacts/sub_packages/roberta.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/datascience/ocw_las_vegas/job_artifacts/sub_packages/roberta.py\n",
    "import tensorflow as tf\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def preprocess(transcription):\n",
    "        new_text = []\n",
    "        for t in transcription.split(\" \"):\n",
    "            t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "            t = 'http' if t.startswith('http') else t\n",
    "            new_text.append(t)\n",
    "        return \" \".join(new_text)\n",
    "    \n",
    "    \n",
    "def roberta_base(new_text):\n",
    "    \n",
    "    text = new_text\n",
    "\n",
    "    #delete models if there (each Job run should not have, but to be sure)\n",
    "    os.system(\"rm -r ./cardiffnlp/twitter-roberta-base-offensive\")\n",
    "    os.system(\"rm -r ./cardiffnlp/twitter-roberta-base-hate\")\n",
    "\n",
    "    #define offensive and hate model\n",
    "    MODEL_OFFENSIVE = f\"cardiffnlp/twitter-roberta-base-offensive\"\n",
    "    MODEL_HATE = f\"cardiffnlp/twitter-roberta-base-hate\"\n",
    "    \n",
    "    #load tokenizers\n",
    "    tokenizer_offensive = AutoTokenizer.from_pretrained(MODEL_OFFENSIVE)\n",
    "    tokenizer_hate = AutoTokenizer.from_pretrained(MODEL_HATE)\n",
    "\n",
    "    #label mapping\n",
    "    labels_offensive=[]\n",
    "    mapping_link_offensive = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/mapping.txt\"\n",
    "    with urllib.request.urlopen(mapping_link_offensive) as f:\n",
    "        html_offensive = f.read().decode('utf-8').split(\"\\n\")\n",
    "        csvreader_offensive = csv.reader(html_offensive, delimiter='\\t')\n",
    "    labels_offensive = [row[1] for row in csvreader_offensive if len(row) > 1]\n",
    "\n",
    "    labels_hate=[]\n",
    "    mapping_link_hate = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/mapping.txt\"\n",
    "    with urllib.request.urlopen(mapping_link_hate) as f:\n",
    "        html_hate = f.read().decode('utf-8').split(\"\\n\")\n",
    "        csvreader_hate = csv.reader(html_hate, delimiter='\\t')\n",
    "    labels_hate = [row[1] for row in csvreader_hate if len(row) > 1]       \n",
    "    \n",
    "    # PT\n",
    "    model_offensive = AutoModelForSequenceClassification.from_pretrained(MODEL_OFFENSIVE)\n",
    "    model_offensive.save_pretrained(MODEL_OFFENSIVE)\n",
    "    \n",
    "    model_hate = AutoModelForSequenceClassification.from_pretrained(MODEL_HATE)\n",
    "    model_hate.save_pretrained(MODEL_HATE)\n",
    "    \n",
    "    #tokenizer text\n",
    "    encoded_input_offensive = tokenizer_offensive(text, return_tensors='pt')\n",
    "    encoded_input_hate = tokenizer_hate(text, return_tensors='pt')\n",
    "    \n",
    "    output_offensive = model_offensive(**encoded_input_offensive)\n",
    "    output_hate = model_hate(**encoded_input_hate)\n",
    "        \n",
    "    scores_offensive = output_offensive[0][0].detach().numpy()\n",
    "    scores_hate = output_hate[0][0].detach().numpy()\n",
    "    \n",
    "    scores_offensive = softmax(scores_offensive)\n",
    "    scores_hate = softmax(scores_hate)\n",
    "\n",
    "    non_offensive = scores_offensive[0]\n",
    "    offensive = scores_offensive[1]\n",
    "    \n",
    "    non_hate = scores_hate[0]\n",
    "    hate = scores_hate[1]\n",
    "\n",
    "    print(\"non_offensive score = \" + str(non_offensive))\n",
    "    print(\"offensive score = \" + str(offensive))\n",
    "    print(\"non_hate score = \" + str(non_hate))\n",
    "    print(\"hate score = \" + str(hate))\n",
    "    \n",
    "    return non_offensive, offensive, non_hate, hate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c84ff",
   "metadata": {},
   "source": [
    "## **4.2 Test script roberta.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64c9e788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_offensive score = 0.76836103\n",
      "offensive score = 0.23163892\n",
      "non_hate score = 0.597862\n",
      "hate score = 0.40213794\n"
     ]
    }
   ],
   "source": [
    "transcription = \"Please leave NOWWW\"\n",
    "\n",
    "new_text = preprocess(transcription)\n",
    "\n",
    "non_offensive, offensive, non_hate, hate = roberta_base(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305da994",
   "metadata": {},
   "source": [
    "# **5. Credentials.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec12c4",
   "metadata": {},
   "source": [
    "## **5.1 Script Credentials.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "49e369d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/datascience/job_artifacts/sub_packages/credentials.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/datascience/ocw_las_vegas/job_artifacts/sub_packages/credentials.py\n",
    "import ads\n",
    "import os\n",
    "import configparser\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "from tempfile import NamedTemporaryFile\n",
    "import urllib\n",
    "import re\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import cx_Oracle\n",
    "\n",
    "def create_uri(user_name, password, wallet_name, wallet_storage_directory):\n",
    "    \n",
    "    database_name = wallet_name\n",
    "    database_user = user_name\n",
    "    database_password = password\n",
    "    \n",
    "    wallet_storage_directory = wallet_storage_directory\n",
    "\n",
    "    # Create the wallet directory if missing: \n",
    "    ads.set_documentation_mode(False)\n",
    "\n",
    "    os.makedirs(wallet_storage_directory, mode=0o700, exist_ok=True)\n",
    "\n",
    "    wallet_path = os.path.join(wallet_storage_directory, database_name)\n",
    "\n",
    "    # Prepare to store ADB connection information\n",
    "    adb_config = os.path.join(wallet_storage_directory, '.credentials')\n",
    "\n",
    "    # Write a configuration file for login creds.\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(adb_config)\n",
    "    config[database_name] = {'tns_admin': wallet_path,\n",
    "                             'sid': '{}_medium'.format(database_name.lower()),\n",
    "                             'user': database_user,\n",
    "                             'password': database_password}\n",
    "    with open(adb_config, 'w') as configfile:\n",
    "        config.write(configfile)\n",
    "\n",
    "\n",
    "    # Read in the credentials configuration files\n",
    "    my_config = configparser.ConfigParser()\n",
    "    my_config.read(adb_config)\n",
    "\n",
    "    # Access a setting\n",
    "    print(my_config[database_name].get('user'))\n",
    "\n",
    "    # Limit the information to a specific database\n",
    "    my_creds = my_config[database_name]\n",
    "    print(my_creds.get('user'))\n",
    "\n",
    "\n",
    "    # extract the wallet\n",
    "    wallet_file = 'Wallet_{}.zip'.format(database_name)\n",
    "    wallet_filename = os.path.join(wallet_storage_directory, wallet_file)\n",
    "    if not os.path.exists(wallet_filename):\n",
    "        print(\"The file {} does not exist.\".format(wallet_filename))\n",
    "        print(\"Please copy the Wallet file, {}, into the directory {} then rerun this cell.\".format(wallet_file, wallet_filename))\n",
    "    else:\n",
    "        os.makedirs(wallet_path, mode=0o700, exist_ok=True)\n",
    "        with ZipFile(wallet_filename, 'r') as zipObj:\n",
    "            zipObj.extractall(wallet_path)\n",
    "\n",
    "\n",
    "    # Update the sqlnet.ora\n",
    "\n",
    "    sqlnet_path = os.path.join(wallet_path, 'sqlnet.ora')\n",
    "    sqlnet_original_path = os.path.join(wallet_path, 'sqlnet.ora.original')\n",
    "    sqlnet_backup_path = os.path.join(wallet_path, 'sqlnet.ora.backup')\n",
    "    if not os.path.exists(sqlnet_original_path):\n",
    "        shutil.copy(sqlnet_path, sqlnet_original_path)\n",
    "    if os.path.exists(sqlnet_path):\n",
    "        shutil.copy(sqlnet_path, sqlnet_backup_path)\n",
    "    sqlnet_re = re.compile('(WALLET_LOCATION\\s*=.*METHOD_DATA\\s*=.*DIRECTORY\\s*=\\s*\\\")(.*)(\\\".*)', \n",
    "                           re.IGNORECASE)\n",
    "    tmp = NamedTemporaryFile()\n",
    "    with open(sqlnet_path, 'rt') as sqlnet:\n",
    "        for line in sqlnet:\n",
    "            tmp.write(bytearray(sqlnet_re.subn(r'\\1{}\\3'.format(wallet_path), line)[0], \n",
    "                                encoding='utf-8'))\n",
    "    tmp.flush()\n",
    "    shutil.copy(tmp.name, sqlnet_path)\n",
    "    tmp.close()\n",
    "\n",
    "    # Add TNS_ADMIN to the environment\n",
    "    os.environ['TNS_ADMIN'] = config[database_name].get('tns_admin')\n",
    "\n",
    "    # Test the database connection\n",
    "    creds = config[database_name]\n",
    "    connect = 'sqlplus ' + creds.get('user') + '/' + creds.get('password') + '@' + creds.get('sid')\n",
    "    print(os.popen(connect).read())\n",
    "\n",
    "    # Get the URI to connect to the database\n",
    "    uri='oracle+cx_oracle://' + creds.get('user') + ':' + creds.get('password') + '@' + creds.get('sid')\n",
    "    \n",
    "    engine = create_engine(uri)\n",
    "\n",
    "    return engine, wallet_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5a985e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb5682",
   "metadata": {},
   "source": [
    "# **6. Script check_table.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67b4c731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/datascience/ocw_las_vegas/job_artifacts/sub_packages/check_table.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/datascience/ocw_las_vegas/job_artifacts/sub_packages/check_table.py\n",
    "\n",
    "def check_table(SCHEMA_NAME, password, wallet_name, wallet_filename):\n",
    "    \n",
    "    import ads\n",
    "    service_name = wallet_name.lower() + \"_high\"\n",
    "    #wallet_location = wallet_storage_directory + \"/\" + \"Wallet_\" + wallet_name + \".zip\"\n",
    "    \n",
    "    print(\"service name is in check_table \" + service_name)\n",
    "    print(\"wallet location is in check_table \" + wallet_filename)\n",
    "\n",
    "    creds = {\"user_name\": SCHEMA_NAME,\n",
    "        \"password\":  password,\n",
    "        \"service_name\": service_name,\n",
    "        \"wallet_location\": wallet_filename}\n",
    "    \n",
    "    print(creds)\n",
    "    \n",
    "    try:\n",
    "        check_table_exists = pd.DataFrame.ads.read_sql(\"SELECT COUNT(*) AS CHECKX FROM ocw_run_results\", connection_parameters=creds)\n",
    "        checkx = check_table_exists['CHECKX'][0]  #checkx will be '1' in table exits\n",
    "        print(\"Table already exist, so append table\")\n",
    "        table_status = 'append'            \n",
    "\n",
    "    except:\n",
    "        table_status = 'replace'\n",
    "        print(\"Table status is replace\")\n",
    "\n",
    "    return table_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_status = check_table(SCHEMA_NAME, password, wallet_name, wallet_storage_directory)\n",
    "table_status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e8eeb5",
   "metadata": {},
   "source": [
    "# **The End**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ocw_las_vegas_v1_v1_0]",
   "language": "python",
   "name": "conda-env-ocw_las_vegas_v1_v1_0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
